{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a190cb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 21:31:41.827276: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749778301.900993    3962 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749778301.922942    3962 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1749778302.078676    3962 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749778302.078714    3962 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749778302.078717    3962 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749778302.078720    3962 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-12 21:31:42.096339: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
    "import time\n",
    "import pandas as pd\n",
    "from tensorflow.keras import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef749be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 GPU(s) detected!\n",
      " - PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "TensorFlow will use the GPU by default when available.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "# List physical devices\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"🚀 GPU(s) detected!\")\n",
    "    for gpu in gpus:\n",
    "        print(f\" - {gpu}\")\n",
    "    print(\"TensorFlow will use the GPU by default when available.\")\n",
    "else:\n",
    "    print(\"⚠️ No GPU detected. Using CPU only.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd1e0d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1749778306.891427    3962 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "/home/paul/Advanced_Deep_Learning_Midterm_Paul/.venv/lib/python3.12/site-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load KMNIST from tensorflow_datasets\n",
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'kmnist',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9437e564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function: flatten images and one-hot encode labels\n",
    "def preprocess(image, label):\n",
    "    image = tf.cast(image, tf.float32) / 255.0         # Normalize to [0,1]\n",
    "    image = tf.reshape(image, [-1])                    # Flatten to 784\n",
    "    label = tf.one_hot(label, depth=10)                # One-hot encode\n",
    "    return image, label\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "ds_train = ds_train.map(preprocess).shuffle(10000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "ds_test = ds_test.map(preprocess).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a357282b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 21:31:47.365009: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:387] The default buffer size is 262144, which is overridden by the user specified `buffer_size` of 8388608\n",
      "2025-06-12 21:31:47.768556: W tensorflow/core/kernels/data/cache_dataset_ops.cc:916] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 21:31:48.002259: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Display one sample image from KMNIST (reshaped for visualization)\n",
    "\n",
    "for image, label in ds_train.take(1):\n",
    "    img = tf.reshape(image[0], (28, 28))  # Take the first image in the batch and reshape\n",
    "    plt.imshow(img.numpy(), cmap='gray')\n",
    "    plt.title(f\"Label: {np.argmax(label[0].numpy())}\")  # Show the class index\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87107c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 21:31:58.380875: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "/home/paul/Advanced_Deep_Learning_Midterm_Paul/.venv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔁 Fold 1/5\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 21:31:59.626049: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 150528000 exceeds 10% of free system memory.\n",
      "2025-06-12 21:32:00.669133: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 150528000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 - 17s - 93ms/step - accuracy: 0.7162 - loss: 0.8868 - val_accuracy: 0.3233 - val_loss: 2.2128 - learning_rate: 1.0000e-03\n",
      "Epoch 2/10\n",
      "188/188 - 16s - 85ms/step - accuracy: 0.8220 - loss: 0.5763 - val_accuracy: 0.6108 - val_loss: 1.7669 - learning_rate: 7.0711e-04\n",
      "Epoch 3/10\n",
      "188/188 - 15s - 78ms/step - accuracy: 0.8443 - loss: 0.5072 - val_accuracy: 0.8648 - val_loss: 0.7796 - learning_rate: 4.0825e-04\n",
      "Epoch 4/10\n",
      "188/188 - 15s - 78ms/step - accuracy: 0.8550 - loss: 0.4731 - val_accuracy: 0.8871 - val_loss: 0.4132 - learning_rate: 2.0412e-04\n",
      "Epoch 5/10\n",
      "188/188 - 15s - 81ms/step - accuracy: 0.8589 - loss: 0.4540 - val_accuracy: 0.8938 - val_loss: 0.3500 - learning_rate: 9.1287e-05\n",
      "Epoch 6/10\n",
      "188/188 - 15s - 78ms/step - accuracy: 0.8632 - loss: 0.4480 - val_accuracy: 0.8952 - val_loss: 0.3426 - learning_rate: 3.7268e-05\n",
      "Epoch 7/10\n",
      "188/188 - 15s - 81ms/step - accuracy: 0.8620 - loss: 0.4444 - val_accuracy: 0.8956 - val_loss: 0.3407 - learning_rate: 1.4086e-05\n",
      "Epoch 8/10\n",
      "188/188 - 15s - 78ms/step - accuracy: 0.8632 - loss: 0.4401 - val_accuracy: 0.8961 - val_loss: 0.3396 - learning_rate: 4.9801e-06\n",
      "Epoch 9/10\n",
      "188/188 - 15s - 79ms/step - accuracy: 0.8657 - loss: 0.4408 - val_accuracy: 0.8961 - val_loss: 0.3395 - learning_rate: 1.6600e-06\n",
      "Epoch 10/10\n",
      "188/188 - 16s - 86ms/step - accuracy: 0.8627 - loss: 0.4432 - val_accuracy: 0.8960 - val_loss: 0.3396 - learning_rate: 5.2495e-07\n",
      "🔁 Fold 2/5\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 21:34:42.617261: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 150528000 exceeds 10% of free system memory.\n",
      "2025-06-12 21:34:42.968074: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 150528000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 - 16s - 84ms/step - accuracy: 0.7171 - loss: 0.8912 - val_accuracy: 0.2095 - val_loss: 2.2177 - learning_rate: 1.0000e-03\n",
      "Epoch 2/10\n",
      "188/188 - 15s - 79ms/step - accuracy: 0.8274 - loss: 0.5584 - val_accuracy: 0.7853 - val_loss: 1.7149 - learning_rate: 7.0711e-04\n",
      "Epoch 3/10\n",
      "188/188 - 15s - 81ms/step - accuracy: 0.8481 - loss: 0.4941 - val_accuracy: 0.8802 - val_loss: 0.7187 - learning_rate: 4.0825e-04\n",
      "Epoch 4/10\n",
      "188/188 - 15s - 77ms/step - accuracy: 0.8612 - loss: 0.4513 - val_accuracy: 0.8977 - val_loss: 0.3709 - learning_rate: 2.0412e-04\n",
      "Epoch 5/10\n",
      "188/188 - 15s - 82ms/step - accuracy: 0.8647 - loss: 0.4373 - val_accuracy: 0.9014 - val_loss: 0.3293 - learning_rate: 9.1287e-05\n",
      "Epoch 6/10\n",
      "188/188 - 17s - 89ms/step - accuracy: 0.8647 - loss: 0.4325 - val_accuracy: 0.9040 - val_loss: 0.3247 - learning_rate: 3.7268e-05\n",
      "Epoch 7/10\n",
      "188/188 - 17s - 90ms/step - accuracy: 0.8691 - loss: 0.4253 - val_accuracy: 0.9028 - val_loss: 0.3216 - learning_rate: 1.4086e-05\n",
      "Epoch 8/10\n",
      "188/188 - 16s - 83ms/step - accuracy: 0.8677 - loss: 0.4260 - val_accuracy: 0.9031 - val_loss: 0.3213 - learning_rate: 4.9801e-06\n",
      "Epoch 9/10\n",
      "188/188 - 16s - 83ms/step - accuracy: 0.8674 - loss: 0.4269 - val_accuracy: 0.9032 - val_loss: 0.3212 - learning_rate: 1.6600e-06\n",
      "Epoch 10/10\n",
      "188/188 - 14s - 77ms/step - accuracy: 0.8683 - loss: 0.4249 - val_accuracy: 0.9035 - val_loss: 0.3211 - learning_rate: 5.2495e-07\n",
      "🔁 Fold 3/5\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 21:37:26.432875: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 150528000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 - 16s - 86ms/step - accuracy: 0.7160 - loss: 0.9038 - val_accuracy: 0.2826 - val_loss: 2.2352 - learning_rate: 1.0000e-03\n",
      "Epoch 2/10\n",
      "188/188 - 15s - 82ms/step - accuracy: 0.8179 - loss: 0.5859 - val_accuracy: 0.6013 - val_loss: 1.7958 - learning_rate: 7.0711e-04\n",
      "Epoch 3/10\n",
      "188/188 - 16s - 83ms/step - accuracy: 0.8430 - loss: 0.5119 - val_accuracy: 0.8438 - val_loss: 0.7809 - learning_rate: 4.0825e-04\n",
      "Epoch 4/10\n",
      "188/188 - 16s - 85ms/step - accuracy: 0.8510 - loss: 0.4798 - val_accuracy: 0.8898 - val_loss: 0.3853 - learning_rate: 2.0412e-04\n",
      "Epoch 5/10\n",
      "188/188 - 16s - 85ms/step - accuracy: 0.8558 - loss: 0.4606 - val_accuracy: 0.8975 - val_loss: 0.3412 - learning_rate: 9.1287e-05\n",
      "Epoch 6/10\n",
      "188/188 - 15s - 79ms/step - accuracy: 0.8579 - loss: 0.4558 - val_accuracy: 0.8985 - val_loss: 0.3334 - learning_rate: 3.7268e-05\n",
      "Epoch 7/10\n",
      "188/188 - 15s - 80ms/step - accuracy: 0.8594 - loss: 0.4528 - val_accuracy: 0.8998 - val_loss: 0.3302 - learning_rate: 1.4086e-05\n",
      "Epoch 8/10\n",
      "188/188 - 14s - 76ms/step - accuracy: 0.8601 - loss: 0.4522 - val_accuracy: 0.8997 - val_loss: 0.3297 - learning_rate: 4.9801e-06\n",
      "Epoch 9/10\n",
      "188/188 - 16s - 83ms/step - accuracy: 0.8607 - loss: 0.4472 - val_accuracy: 0.8999 - val_loss: 0.3296 - learning_rate: 1.6600e-06\n",
      "Epoch 10/10\n",
      "188/188 - 14s - 77ms/step - accuracy: 0.8616 - loss: 0.4503 - val_accuracy: 0.8997 - val_loss: 0.3296 - learning_rate: 5.2495e-07\n",
      "🔁 Fold 4/5\n",
      "Epoch 1/10\n",
      "188/188 - 16s - 87ms/step - accuracy: 0.7245 - loss: 0.8780 - val_accuracy: 0.1965 - val_loss: 2.2180 - learning_rate: 1.0000e-03\n",
      "Epoch 2/10\n",
      "188/188 - 16s - 83ms/step - accuracy: 0.8264 - loss: 0.5643 - val_accuracy: 0.6006 - val_loss: 1.7552 - learning_rate: 7.0711e-04\n",
      "Epoch 3/10\n",
      "188/188 - 16s - 85ms/step - accuracy: 0.8472 - loss: 0.4928 - val_accuracy: 0.8466 - val_loss: 0.7706 - learning_rate: 4.0825e-04\n",
      "Epoch 4/10\n",
      "188/188 - 16s - 86ms/step - accuracy: 0.8572 - loss: 0.4592 - val_accuracy: 0.8880 - val_loss: 0.3866 - learning_rate: 2.0412e-04\n",
      "Epoch 5/10\n",
      "188/188 - 17s - 88ms/step - accuracy: 0.8626 - loss: 0.4481 - val_accuracy: 0.8955 - val_loss: 0.3388 - learning_rate: 9.1287e-05\n",
      "Epoch 6/10\n",
      "188/188 - 16s - 86ms/step - accuracy: 0.8662 - loss: 0.4355 - val_accuracy: 0.8942 - val_loss: 0.3324 - learning_rate: 3.7268e-05\n",
      "Epoch 7/10\n",
      "188/188 - 16s - 86ms/step - accuracy: 0.8645 - loss: 0.4354 - val_accuracy: 0.8955 - val_loss: 0.3302 - learning_rate: 1.4086e-05\n",
      "Epoch 8/10\n",
      "188/188 - 15s - 82ms/step - accuracy: 0.8673 - loss: 0.4300 - val_accuracy: 0.8962 - val_loss: 0.3297 - learning_rate: 4.9801e-06\n",
      "Epoch 9/10\n",
      "188/188 - 16s - 87ms/step - accuracy: 0.8667 - loss: 0.4366 - val_accuracy: 0.8964 - val_loss: 0.3296 - learning_rate: 1.6600e-06\n",
      "Epoch 10/10\n",
      "188/188 - 15s - 78ms/step - accuracy: 0.8643 - loss: 0.4333 - val_accuracy: 0.8964 - val_loss: 0.3296 - learning_rate: 5.2495e-07\n",
      "🔁 Fold 5/5\n",
      "Epoch 1/10\n",
      "188/188 - 16s - 84ms/step - accuracy: 0.7056 - loss: 0.9149 - val_accuracy: 0.3557 - val_loss: 2.2231 - learning_rate: 1.0000e-03\n",
      "Epoch 2/10\n",
      "188/188 - 15s - 78ms/step - accuracy: 0.8170 - loss: 0.5845 - val_accuracy: 0.5512 - val_loss: 1.7435 - learning_rate: 7.0711e-04\n",
      "Epoch 3/10\n",
      "188/188 - 16s - 86ms/step - accuracy: 0.8408 - loss: 0.5098 - val_accuracy: 0.8433 - val_loss: 0.7585 - learning_rate: 4.0825e-04\n",
      "Epoch 4/10\n",
      "188/188 - 15s - 79ms/step - accuracy: 0.8537 - loss: 0.4716 - val_accuracy: 0.8934 - val_loss: 0.3731 - learning_rate: 2.0412e-04\n",
      "Epoch 5/10\n",
      "188/188 - 15s - 80ms/step - accuracy: 0.8588 - loss: 0.4567 - val_accuracy: 0.8966 - val_loss: 0.3375 - learning_rate: 9.1287e-05\n",
      "Epoch 6/10\n",
      "188/188 - 15s - 77ms/step - accuracy: 0.8632 - loss: 0.4431 - val_accuracy: 0.8982 - val_loss: 0.3311 - learning_rate: 3.7268e-05\n",
      "Epoch 7/10\n",
      "188/188 - 16s - 85ms/step - accuracy: 0.8620 - loss: 0.4445 - val_accuracy: 0.8989 - val_loss: 0.3286 - learning_rate: 1.4086e-05\n",
      "Epoch 8/10\n",
      "188/188 - 15s - 77ms/step - accuracy: 0.8649 - loss: 0.4392 - val_accuracy: 0.8988 - val_loss: 0.3279 - learning_rate: 4.9801e-06\n",
      "Epoch 9/10\n",
      "188/188 - 15s - 82ms/step - accuracy: 0.8631 - loss: 0.4393 - val_accuracy: 0.8988 - val_loss: 0.3279 - learning_rate: 1.6600e-06\n",
      "Epoch 10/10\n",
      "188/188 - 15s - 78ms/step - accuracy: 0.8637 - loss: 0.4385 - val_accuracy: 0.8984 - val_loss: 0.3279 - learning_rate: 5.2495e-07\n",
      "   Fold  Accuracy      Loss  Training Time (s)\n",
      "0     1  0.896000  0.339607         155.092937\n",
      "1     2  0.903500  0.321099         156.211560\n",
      "2     3  0.899667  0.329585         154.335638\n",
      "3     4  0.896417  0.329564         160.086909\n",
      "4     5  0.898417  0.327909         152.533014\n"
     ]
    }
   ],
   "source": [
    "# Prepare full dataset as numpy arrays for cross-validation\n",
    "images = []\n",
    "labels = []\n",
    "results = []\n",
    "\n",
    "for image, label in tfds.as_numpy(ds_train.unbatch()):\n",
    "    images.append(image.flatten() / 255.0)\n",
    "    labels.append(label) \n",
    "    \n",
    "X = np.array(images, dtype=np.float32)\n",
    "y = np.stack(labels).astype(np.float32)\n",
    "\n",
    "# Learning rate scheduler\n",
    "def scheduler(epoch, lr):\n",
    "    return lr / np.sqrt(epoch + 1)\n",
    "\n",
    "lr_callback = LearningRateScheduler(scheduler)\n",
    "\n",
    "# K-Fold Cross-Validation setup\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X)):\n",
    "    print(f\"🔁 Fold {fold+1}/5\")\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(784,)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Optimizer setup\n",
    "    optimizer = Adam(\n",
    "        learning_rate=1e-3,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-7\n",
    "        )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model.fit(\n",
    "        X[train_index], y[train_index],\n",
    "        validation_data=(X[val_index], y[val_index]),\n",
    "        epochs=10,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=[lr_callback],\n",
    "        verbose=2\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    val_loss, val_accuracy = model.evaluate(X[val_index], y[val_index], verbose=0)\n",
    "\n",
    "    # Store results\n",
    "    results.append({\n",
    "        'Fold': fold + 1,\n",
    "        'Accuracy': val_accuracy,\n",
    "        'Loss': val_loss,\n",
    "        'Training Time (s)': elapsed_time\n",
    "    })\n",
    "\n",
    "# Create and display table\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
